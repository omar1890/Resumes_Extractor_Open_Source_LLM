{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import json"
      ],
      "metadata": {
        "id": "9xvoewHPqYfP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Recommended Model Sizes for Colab Pro:**\n",
        "| Model Type | Parameters (Billions) | VRAM Required (GB) | RAM Required (GB) | Notes |\n",
        "|------------|----------------------|--------------------|-------------------|-------|\n",
        "| **Tiny LLM** | 3B - 7B | 5-10 GB | 8-12 GB | Fastest on Colab GPUs (T4, P100) |\n",
        "| **Mid-size LLM** | 13B - 20B | 12-16 GB | 16+ GB | Needs A100 (Colab Pro+) |\n",
        "| **Large LLM** | 30B+ | 24-40 GB | 32+ GB | Only runs efficiently on A100 40GB |\n",
        "\n",
        "---\n",
        "\n",
        "### **Colab GPU Tiers and LLM Feasibility:**\n",
        "| Colab Plan | GPU (Varies) | VRAM | Suitable Max Model Size |\n",
        "|------------|-------------|------|--------------------------|\n",
        "| **Colab Free** | T4 / P100 | 12-16 GB | 7B (Possibly 13B with quantization) |\n",
        "| **Colab Pro** | T4 / V100 | 16-24 GB | 13B-20B (8-bit quantized) |\n",
        "| **Colab Pro+** | A100 40GB | 40 GB | 30B+ (best for large models) |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uc7DZ09Wrx3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Ollama"
      ],
      "metadata": {
        "id": "fxPCbt7EqckL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install LLM models"
      ],
      "metadata": {
        "id": "3uj4xePIeAq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H512s9RtqYcb",
        "outputId": "d9e50a8a-c44b-400c-a4f2-cd354d5e7283"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13281    0 13281    0     0  58390      0 --:--:-- --:--:-- --:--:-- 58506\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup bash -c \"OLLAMA_HOST=0.0.0.0:7000 OLLAMA_ORIGIN=* ollama serve\" &\n",
        "!sleep 5 && tail /content/nohup.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE3nuIFaqYZz",
        "outputId": "0eb9cc71-8cd5-4e64-fe76-1af4ee0b5019"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "llama_init_from_model:  CUDA_Host  output buffer size =     0.82 MiB\n",
            "llama_init_from_model:      CUDA0 compute buffer size =   563.00 MiB\n",
            "llama_init_from_model:  CUDA_Host compute buffer size =    21.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1225\n",
            "llama_init_from_model: graph splits = 2\n",
            "time=2025-04-17T22:21:10.418Z level=INFO source=server.go:619 msg=\"llama runner started in 0.75 seconds\"\n",
            "[GIN] 2025/04/17 - 22:21:14 | 200 |  5.641585285s | 156.208.237.171 | POST     \"/api/chat\"\n",
            "[GIN] 2025/04/17 - 22:23:32 | 200 | 13.749960707s | 156.208.237.171 | POST     \"/api/chat\"\n",
            "[GIN] 2025/04/17 - 22:24:50 | 200 |  3.195823834s | 156.208.237.171 | POST     \"/api/chat\"\n",
            "Error: listen tcp 0.0.0.0:7000: bind: address already in use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env OLLAMA_HOST=0.0.0.0:7000\n",
        "%env OLLAMA_ORIGIN=*\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTMzBRqiqYW8",
        "outputId": "73378031-966b-40f6-d5d1-423628b335bc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: OLLAMA_HOST=0.0.0.0:7000\n",
            "env: OLLAMA_ORIGIN=*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay3p54KQpmFp",
        "outputId": "8daeb8e3-433a-4a00-9c8a-880af088165b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0.0.0:7000\n",
            "nohup.out sample_data\n"
          ]
        }
      ],
      "source": [
        "!echo $OLLAMA_HOST\n",
        "!echo $OLLAMA_ORIGIN\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ollama pull llama2:13b"
      ],
      "metadata": {
        "id": "xePrntcjNdTN",
        "collapsed": true
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PRgIb4DpsfYh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull phi"
      ],
      "metadata": {
        "id": "Onnq4MjhsfHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2248d46-3eb9-42a3-9828-5538d4ed105d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
            "pulling 04778965089b... 100% ▕▏ 1.6 GB                         \u001b[K\n",
            "pulling 7908abcab772... 100% ▕▏ 1.0 KB                         \u001b[K\n",
            "pulling 774a15e6f1e5... 100% ▕▏   77 B                         \u001b[K\n",
            "pulling 3188becd6bae... 100% ▕▏  132 B                         \u001b[K\n",
            "pulling 0b8127ddf5ee... 100% ▕▏   42 B                         \u001b[K\n",
            "pulling 4ce4b16d33a3... 100% ▕▏  555 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull mistral"
      ],
      "metadata": {
        "id": "CcjaOPJfse-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2363d5-682d-41b0-b07a-4e826fe9f95c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
            "pulling ff82381e2bea... 100% ▕▏ 4.1 GB                         \u001b[K\n",
            "pulling 43070e2d4e53... 100% ▕▏  11 KB                         \u001b[K\n",
            "pulling 491dfa501e59... 100% ▕▏  801 B                         \u001b[K\n",
            "pulling ed11eda7790d... 100% ▕▏   30 B                         \u001b[K\n",
            "pulling 42347cd80dc8... 100% ▕▏  485 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3"
      ],
      "metadata": {
        "id": "anPAXpIk99Jq",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de3c4336-e30f-4d44-c62c-c54bff935ed0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
            "pulling 6a0746a1ec1a... 100% ▕▏ 4.7 GB                         \u001b[K\n",
            "pulling 4fa551d4f938... 100% ▕▏  12 KB                         \u001b[K\n",
            "pulling 8ab4849b038c... 100% ▕▏  254 B                         \u001b[K\n",
            "pulling 577073ffcc6c... 100% ▕▏  110 B                         \u001b[K\n",
            "pulling 3f8eb4da87fa... 100% ▕▏  485 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ngrok"
      ],
      "metadata": {
        "id": "GW5wC9zY6Oji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiuoeT46RKmg",
        "outputId": "0f6120e8-9533-4291-c3b6-5e35ddcd5c0b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                 ID              SIZE      MODIFIED               \n",
            "llama3:latest        365c0bd3c000    4.7 GB    Less than a second ago    \n",
            "mistral:latest       f974a74358d6    4.1 GB    Less than a second ago    \n",
            "phi:latest           e2fd6321a5fe    1.6 GB    1 second ago              \n",
            "all-minilm:l12-v2    4f5da3bd944d    67 MB     About an hour ago         \n",
            "llama2:13b           d475bf4c50bc    7.4 GB    About an hour ago         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbrKaLLQ6TWp",
        "outputId": "f1b0bcd6-398d-4101-9df6-ee036538465e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.4)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "ngrok_auth = userdata.get('colab-ngrok')\n",
        "\n",
        "conf.get_default().auth_token = ngrok_auth\n",
        "\n",
        "port = \"7000\"\n",
        "\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-EvJqqA6V0p",
        "outputId": "ffc018eb-c976-4e26-81ab-5a2789b52464"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://8216-34-90-52-240.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYpH_KKO3B_k",
        "outputId": "654aebfa-febf-4d8b-dec2-a65f56c41bad"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                 ID              SIZE      MODIFIED          \n",
            "llama3:latest        365c0bd3c000    4.7 GB    3 seconds ago        \n",
            "mistral:latest       f974a74358d6    4.1 GB    3 seconds ago        \n",
            "phi:latest           e2fd6321a5fe    1.6 GB    4 seconds ago        \n",
            "all-minilm:l12-v2    4f5da3bd944d    67 MB     About an hour ago    \n",
            "llama2:13b           d475bf4c50bc    7.4 GB    About an hour ago    \n"
          ]
        }
      ]
    }
  ]
}